{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json('./friends-1-227-Rachel-pair.json')"
      ],
      "metadata": {
        "id": "AncFLNhWsoeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UD2MqKjnsnvc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "def train():\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "    # Tokenize the conversations and create input/output pairs\n",
        "    input_ids = []\n",
        "    output_ids = []\n",
        "\n",
        "    inputs = df['question']\n",
        "    outputs = df['answer']\n",
        "\n",
        "    for i in range(df.shape[0] - 1):\n",
        "        input_text = \"<|startoftext|>\" + inputs[i] + \"<|endoftext|>\"\n",
        "        output_text = outputs[i]\n",
        "\n",
        "        input_tokenized = tokenizer.encode(input_text,\n",
        "                add_special_tokens=False)\n",
        "        output_tokenized = tokenizer.encode(output_text,\n",
        "                add_special_tokens=False)\n",
        "        input_ids.append(input_tokenized)\n",
        "        output_ids.append(output_tokenized)\n",
        "\n",
        "    # Pad the input/output pairs to the same length\n",
        "    max_length = max(len(ids) for ids in input_ids + output_ids)\n",
        "\n",
        "    input_ids = \\\n",
        "        tf.keras.preprocessing.sequence.pad_sequences(input_ids,\n",
        "            maxlen=max_length, padding='post')\n",
        "    output_ids = \\\n",
        "        tf.keras.preprocessing.sequence.pad_sequences(output_ids,\n",
        "            maxlen=max_length, padding='post')\n",
        "\n",
        "    # Define the training parameters\n",
        "    batch_size = 16\n",
        "    epochs = 3\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss=model.hf_compute_loss)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(input_ids, output_ids, batch_size=batch_size,\n",
        "              epochs=epochs)\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save_pretrained('chatbot_model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def test():\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "  model = TFGPT2LMHeadModel.from_pretrained(\"chatbot_model\")\n",
        "\n",
        "  input_ids = []\n",
        "  output_ids = []\n",
        "\n",
        "  inputs = df['question']\n",
        "  outputs = df['answer']\n",
        "\n",
        "  inputs = df['question']\n",
        "  outputs = df['answer']\n",
        "\n",
        "  for i in range(df.shape[0] - 1):\n",
        "      input_text = inputs[i]\n",
        "      output_text = outputs[i]\n",
        "\n",
        "      input_tokenized = tokenizer.encode(input_text,\n",
        "              add_special_tokens=False)\n",
        "      output_tokenized = tokenizer.encode(output_text,\n",
        "              add_special_tokens=False)\n",
        "      input_ids.append(input_tokenized)\n",
        "      output_ids.append(output_tokenized)\n",
        "\n",
        "  # Pad the input/output pairs to the same length\n",
        "  max_length = max(len(ids) for ids in input_ids + output_ids)\n",
        "\n",
        "  while True:\n",
        "    input_text = input(\"User: \")\n",
        "    input_text = \"<|startoftext|>\" + input_text + \"<|endoftext|>\"\n",
        "    input_tokenized = tokenizer.encode(input_text, add_special_tokens=False)\n",
        "    input_ids = tf.keras.preprocessing.sequence.pad_sequences([input_tokenized], maxlen=max_length, padding=\"post\")\n",
        "\n",
        "    attention_mask = np.where(input_ids != 0, 1, 0)  # Dikkat maskesi oluştur\n",
        "    output_ids = model.generate(input_ids, max_length=max_length + 1, attention_mask=attention_mask, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Burada gereksiz tekrarları ve anlamsız tokenları filtreleyebilirsiniz\n",
        "\n",
        "    print(\"Bot:\", output_text)"
      ],
      "metadata": {
        "id": "HUggXypnvvCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KZ7opwXtAfh",
        "outputId": "8167f2fd-ce0e-4be2-eefb-e34163469c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "285/285 [==============================] - 74s 139ms/step - loss: 1.4485\n",
            "Epoch 2/3\n",
            "285/285 [==============================] - 40s 140ms/step - loss: 1.3675\n",
            "Epoch 3/3\n",
            "285/285 [==============================] - 40s 140ms/step - loss: 1.3528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "id": "aNZpD5IWtFqN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "outputId": "61ab8cf3-0e70-4e68-a787-4d8797a634e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at chatbot_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: <|startoftext|>hi!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!.\n",
            "User: hello\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: <|startoftext|>hello!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-fbd55f77ab7c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-d51a56ce3244>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"<|startoftext|>\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_text\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"<|endoftext|>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0minput_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0xtVHUdUw7ZH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}